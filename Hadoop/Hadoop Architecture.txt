Introduction
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. 
It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. 
HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is 
suitable for applications that have large data sets.

Assumptions and Goals
1. Hardware Failure
Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, 
each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of 
failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core 
architectural goal of HDFS.

2. Streaming Data Access
Applications that run on HDFS need streaming access to their data sets. They are not general purpose applications that typically run on general purpose file systems. 
HDFS is designed more for batch processing rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of 
data access.

3. Large Data Sets
Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. 
It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.

4. Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed except for appends and truncates. 
Appending the content to the end of the files is supported but cannot be updated at arbitrary point. This assumption simplifies data coherency issues and enables 
high throughput data access. A MapReduce application or a web crawler application fits perfectly with this model.

5. “Moving Computation is Cheaper than Moving Data”
A computation requested by an application is much more efficient if it is executed near the data it operates on. This is especially true when the size of the data 
set is huge. This minimizes network congestion and increases the overall throughput of the system. The assumption is that it is often better to migrate the computation 
closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves 
closer to where the data is located.
