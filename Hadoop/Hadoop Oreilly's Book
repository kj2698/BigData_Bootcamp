The version 2 release made significant leaps compared to version 1 of Hadoop. It introduced YARN, a sophisticated general-purpose
resource manager and job scheduling component. HDFS high availability, HDFS federations, and HDFS snapshots were some other prominent features introduced 
in version 2 releases.

The core idea behind the MapReduce model was to provide parallelism, fault tolerance, and data locality features. Data locality means a program is 
executed where data is stored instead of bringing the data to the program.

NameNodes and DataNodes have a specific role in managing overall clusters. NameNodes are responsible for maintaining metadata information.
MapReduce engines have a job tracker and task tracker whose scalability is limited to 40,000 nodes because the overall work of scheduling and 
tracking is handled by only the job tracker. YARN was introduced in Hadoop version 2 to overcome scalability issues and resource management jobs. 
It gave Hadoop a new lease of life and Hadoop became a more robust, faster, and more scalable system.

---------------------------------------------------------
Overview of Hadoop 3 and its features
The first alpha release of Hadoop version 3.0.0 was on 30 August 2016. It was called version 3.0.0-alpha1. This was the first alpha release in a series of planned
alphas and betas that ultimately led to 3.0.0 GA. The intention behind this alpha release was to quickly gather and act on feedback from downstream users.

With any such releases, there are some key drivers that lead to its birth. These key drivers create benefits that will ultimately help in the better functioning 
of Hadoop-augmented enterprise applications. Before we discuss the features of Hadoop 3, you should understand these driving factors. Some driving factors behind 
the release of Hadoop 3 are as follows:

A lot of bug fixes and performance improvements: Hadoop has a growing open source community of developers regularly adding major/minor changes or improvements to 
the Hadoop trunk repository. These changes were growing day by day and they couldn't be accommodated in minor version releases of 2.x. They had to be accommodated 
with a major version release. Hence, it was decided to release the majority of these changes committed to the trunk repository with Hadoop 3.
Overhead due to data replication factor: As you may be aware, HDFS has a default replication factor of 3. This helps make things more fault-tolerant with better 
data locality and better load balancing of jobs among DataNodes. However, it comes with an overhead cost of around 200%. For non-frequently accessed datasets 
that have low I/O activities, these replicated blocks are never accessed in the course of normal operations. On the other hand, they consume the same number of 
resources as other main resources. To mitigate this overhead with non-frequently accessed data, Hadoop 3 introduced a major feature, called erasure coding. 
This stores data durably while saving space significantly.
Improving existing YARN Timeline services: YARN Timeline service version 1 has limitations that impact reliability, performance, and scalability. For example, 
it uses local-disk-based LevelDB storage that cannot scale to a high number of requests. Moreover, the Timeline server is a single point of failure. To mitigate 
such drawbacks, YARN Timeline server has been re-architected with the Hadoop 3 release.
Optimizing map output collector: It is a well-known fact that native code (written correctly) is faster to execute. In lieu of that, some optimization is done 
in Hadoop 3 that will speed up mapper tasks by approximately two to three times. The native implementation of map output collector has been added, which will
be used in the Java-based MapReduce framework using the Java Native Interface (JNI). This is particularly useful for shuffle-intensive operations.
The need for a higher availability factor of NameNode: Hadoop is a fault-tolerant platform with support for handling multiple data node failures. In the case
of NameNodes versions, prior to Hadoop version 3 support two NameNodes, Active and Standby. While it is a highly available solution, in the case of the failure 
of an active (or standby) NameNode, it will go back to a non-HA mode. This is not very accommodative of a high number of failures. In Hadoop 3, support for more
than one standby NameNode has been introduced.
Dependency on Linux ephemeral port range: Linux ephemeral ports are short-lived ports created by the OS (operating system) when a process requests any available port.
The OS assigns the port number from a predefined range. It then releases the port after the related connection terminates. With version 2 and earlier, many Hadoop 
services' default ports were in the Linux ephemeral port range. This means starting these services sometimes failed to bind to the port due to conflicts with 
other processes. In Hadoop 3, these default ports are moved out of the ephemeral port range.
Disk-level data skew: There are multiple disks (or drives) managed by DataNodes. Sometimes, adding or replacing disks leads to significant data skew within 
a DataNode. To rebalance data among disks within a DataNode, Hadoop 3 has introduced a CLI utility called hdfs diskbalancer.
Well! Hopefully, by now, you have a clear understanding of why certain features were introduced in Hadoop 3 and what kinds of benefits are derived from them. 

-------------------------------------------------

Defining HDFS
HDFS is designed to run on a cluster of commodity hardware. It is a fault-tolerant, scalable File System that handles the failure of nodes without
data and can scale up horizontally to any number of nodes. The initial goal of HDFS was to serve large data files with high read and write performance.

The following are a few essential features of HDFS:

Fault tolerance: Downtime due to machine failure or data loss could result in a huge loss to a company; therefore, the companies want a highly available 
fault-tolerant system. HDFS is designed to handle failures and ensures data availability with corrective and preventive actions.
Files stored in HDFS are split into small chunks and each chunk is referred to as a block. Each block is either 64 MB or 128 MB, depending on the configuration. 
Blocks are replicated across clusters based on the replication factor. This means that if the replication factor is three, then the block will be replicated to
three machines. This assures that, if a machine holding one block fails, the data can be served from another machine.

Streaming data access: HDFS works on a write once read many principle. Data within a file can be accessed by an HDFS client. Data is served in the form of streams, 
which means HDFS enables streaming access to large data files where data is transferred as a continuous stream. HDFS does not wait for the entire file to be 
read before sending data to the client; instead, it sends data as soon as it reads it. The client can immediately process the received stream, which makes 
data processing efficient.

Scalability: HDFS is a highly scalable File System that is designed to store a large number of big files and allows you to add any number of machines to 
increase its storage capability. Storing a huge number of small files is generally not recommended; the size of the file should be equal to or greater than 
the block size. Small files consume more RAM space on master nodes, which may decrease the performance of HDFS operations.

Simplicity: HDFS is easy to set up and manage. It is written in Java. It provides easy command-line interface tools that are very much similar to Linux commands. 
Later in this chapter, we will see how easy it is to operate HDFS via a command-line utility. 

High availability: HDFS is a highly available distributed File System. Every read and write request goes to a master node, and a master node can be a 
single point of failure. Hadoop offers the high availability feature, which means a read and write request will not be affected by the failure of the active 
master node. When the active master node fails, the standby master node takes over. In Hadoop version 3, we can have more than two master nodes running at 
once to make high availability more robust and efficient.
-------------------------------------------------

Deep dive into the HDFS architecture
As a big data practitioner or enthusiast, you must have read or heard about the HDFS architecture. The goal of this section is to explore the 
architecture in depth, including the main and essential supporting components. By the end of this section, you will have a deep knowledge of 
the HDFS architecture, along with the intra-process communication of architecture components. But first, let's start by establishing definition 
of HDFS (Hadoop Distributed File System). HDFS is the storage system of the Hadoop platform, which is distributed, fault-tolerant, 
and immutable in nature. HDFS is specifically developed for large datasets (too large to fit in cheaper commodity machines). 
Since HDFS is designed for large datasets on commodity hardware, it purposely mitigates some of the bottlenecks associated with large datasets.

We will understand some of these bottlenecks and how HDFS mitigates them here:

With large datasets comes the problem of slow processing they are run on only one computer. The Hadoop platform consists of two 
logical components, distributed storage and distributed processing. HDFS provides distributed storage. MapReduce and other YARN-compatible 
frameworks provide distributed processing capabilities. To mitigate this, Hadoop offers distributed data processing, which has several 
systems processing a chunk of data simultaneously. 

With distributed processing of large datasets, one of the challenges is to mitigate large data movements over the network. HDFS makes
 provisions for applications or code to move the computation closer to where the data is located. This ensures less utilization of cluster 
 bandwidth. Moreover, data in HDFS is replicated by default and each replica is hosted by a different node. This replication helps in moving
 computation closer to the data. For example, if the node hosting one of the replicas of the HDFS block is busy and does not have any open 
 slots for running jobs, then the computation would be moved to another node hosting some other HDFS block replica.
 
With large datasets, the cost of failure is greater. So, if a complex process on large datasets (running over a longer duration) fails, 
then rerunning that complex data processing job is significant in terms of resource costs and time consumption. Moreover, one of the side 
effects of distributed processing is that the chances of failure are high due to high network communication and coordination across a large 
number of machines. Lastly, it runs on commodity hardware, where failure is unavoidable. To mitigate such risks, HDFS is built with an automated 
mechanism to detect and recover from faults.

HDFS is designed to be a File System that is used for multiple access by end users and processes in a distributed cluster. In the case of 
multiple random access, having provisions for modifying files at arbitrary positions is error-prone and difficult to manage. To mitigate this 
risk, HDFS is designed to support a simple coherency model where the file is not allowed to be modified at arbitrary points once it has been 
written, created, and closed for the first time. You can only add content at the end of a file or truncate it completely. This simple 
coherency model keeps the HDFS design simple, scalable, and less buggy. 
---------------------------------------------------

HDFS logical architecture
We'll now gain an understanding of some of the design decisions of HDFS and how they mitigate some of the bottlenecks associated 
with a large dataset's storage and processing in a distributed manner. It's time to take a deep dive into the HDFS architecture. 
The following diagram represents the logical components of HDFS:




For simplicity's sake, you can divide the architecture into two groups. One group can be called the data group. 
It consists of processes/components that are related to file storage. The other group can be called the management group. 
It consists of processes/components that are used to manage data operations such as read, write, truncate, and delete.

So, the data group is about data blocks, replication, checkpoints, and file metadata. The management group is about NameNodes, 
DataNodes, JournalNodes, and Zookeepers. We will first take at the management group's components and then we will talk about the data group's components:

NameNode: HDFS is a master-slave architecture. The NameNode plays the role of a master in the HDFS architecture. It is the regulator that controls 
all operations on the data and stores all relevant metadata about data that's stored in HDFS. All data operations will first go through a NameNode and 
then to other relevant Hadoop components. The NameNode manages the File System namespace. It stores the File System tree and metadata of files and 
directories. All of this information is stored on the local disk in three types of files, namely File System namespace, image (fsimage) files, and edit logs files.

The fsimage file stores the state of the File System at a point in time. The edit logs files contains a list of all changes (creation, modification, 
truncation, or deletion) that are made to each HDFS file after the last fsimage file was created. A new fsimage file is created after collaborating 
the content of the most recent fsimage files with the latest edit logs. This process of merging fsimage files with edit logs is called checkpointing.
 It is system-triggered and is managed by system policies. NameNode also maintains a mapping of all data blocks to DataNode.
 
DataNode: DataNodes plays the role of slaves in the HDFS architecture. They perform data block operations (creation, modification, or deletion)
 based on instructions that are received from NameNodes or HDFS clients. They host data processing jobs such as MapReduce. They report back 
 block information to NameNodes. DataNodes also communicate between each other in the case of data replication.
 
JournalNode: With NameNode high availability, there was a need to manage edit logs and HDFS metadata between a active and standby NameNodes. 
JournalNodes were introduced to efficiently share edit logs and metadata between two NameNodes. JournalNodes exercise concurrency write locks 
to ensure that edit logs are written by one active NameNode at a time. This level of concurrency control is required to avoid the state of a 
NameNode from being managed by two different services that act as failovers of one another at the same time. This type of scenario, where edit
 logs are managed by two services at the same time, is called HDFS split brain scenario, and it can result in data loss or inconsistent state. 
 JournalNodes avoid such scenarios by allowing only one NameNode to be writing to edit logs at a time.
 
Zookeeper failover controllers: With the introduction of high availability (HA) in NameNodes, automatic failover was introduced as well. 
HA without automatic failover would have manual intervention to bring NameNode services back up in the event of failure. This is not ideal. 
Hence, the Hadoop community has introduced two components: Zookeeper Quorum and Zookeeper Failover controller, also known as ZKFailoverController (ZKFC). 
Zookeeper maintains data about NameNode health and connectivity. It monitors clients and notifies other clients in the event of failure. 
Zookeeper maintains an active persistent session with each of the NameNodes, and this session is renewed by each of them upon expiry. 
In the event of a failure or crash, the expired session is not renewed by the failed NameNode. This is when Zookeeper informs other 
standby NameNodes to initiate the failover process. Every NameNode server has a Zookeeper client installed on it. This Zookeeper 
client is called ZKFC. Its prime responsibilities are monitoring the health of the NameNode processes, managing sessions with Zookeeper servers, 
and acquiring write lock concurrency in the case of its local NameNode being active. ZKFC monitors NameNode health with periodic health check pings. 
If a NameNode responds to those pings in a timely manner, then ZKFC considers that NameNode to be healthy. If not, then ZKFC considers it to 
be unhealthy and accordingly notifies the Zookeeper servers about it. In the case of the local NameNode being healthy and active, ZKFC opens 
a session in Zookeeper and creates a lock znode on the Zookeeper servers. This znode is ephemeral in nature and will be deleted
 automatically when the session expires.
 --------------------------------------------------
 
 Blocks
Blocks define the minimum amount of data that HDFS can read and write at a time. HDFS, when storing large files, divides them into sets of individual 
blocks and stores each of these blocks on different data nodes in a Hadoop cluster. All files are divided into data blocks and then stored in HDFS. 
The default value of a HDFS block size is either 64 MB or 128 MB. This is large compared to Unix-level File System blocks. Having a large HDFS data 
block size is beneficial in the case of storing and processing large volumes of data in Hadoop. One of the reasons for this is to efficiently manage
the metadata associated with each data block. If the size of the data blocks are too small, then more metadata will be stored in NameNodes, causing 
its RAM to be filled up quickly. This will also result in more remote procedural calls (RPCs) to NameNode ports, which may result in resource contention.

The other reason is that large data blocks would result in higher Hadoop throughput. With an appropriate data block size, you can strike a balance 
between how many data nodes would be running parallel processes to perform operations on a given dataset and how much data can be processed by an 
individual process given the amount of resources allocated to it. Larger data blocks also result in less time being spent in disk-seeking operations
or finding out the start of the data block. In addition to the advantages of having a large HDFS block size, the concept of HDFS block abstraction 
have other advantages in Hadoop operations. One such benefit is that you can store files larger than the size of a disk of an individual machine.
The other benefit is that it provides better replication strategy and failover. Corrupted disk blocks can be easily replaced by replicated blocks
from some other DataNode.
-------------------------------------

Replication
HDFS replication is critical for reliability, scalability, and performance. By default, HDFS has a replication factor of three. Therefore, Hadoop creators
have given careful thought on where each data block replica should be placed. It is all policy-driven. The current implementation follows the rack-aware 
replica management policy. Before we look at that in detail, we should first go through some of the facts about server racks. Any communication between 
two racks goes through switches, and the available network bandwidth between two racks is generally less than the bandwidth between machines on the same rack. 
Large Hadoop clusters spread across multiple racks. Hadoop tries to place replicas onto different racks. This prevents data loss in the case of an entire 
rack unit failing and utilizes multiple available rack bandwidth for reading data.
However, this increases write latency as data needs to be transferred to multiple racks. If the replication factor is three, then HDFS would put one replica 
on the local machine where the writer is present, otherwise it would put on a random DataNode. Another replica would be placed on the DataNode on a different
remote rack, and the last replica would be placed on another DataNode in the same remote rack. The HDFS replication policy makes an assumption that rack 
failures are less probable than node failures. A block is only placed in two different racks, not three, which reduces the probability of network bandwidth 
being used. The current replication policy does not equally distribute files across racks—if the replication factor is three, then two replicas will 
be on the same rack and the third one will be on another rack. In bigger clusters, we may have more replication factors. In such cases, two thirds of 
the replicas will be placed on one rack, one thirds of the replicas will be placed on another block, and the rest of the replicas will be equally 
distributed between the remaining racks.

The maximum number of replicas that we can have on HDFS is equal to the number of DataNodes because a DataNode cannot keep multiple copies of the same block. 
HDFS always tries for direct read requests to the replica that's closest to the client. If the reader node is under the same rack where the replica is, 
then it is assigned for reading the block. If the replication factor is more than the default replication factor, which is three, then the fourth 
and following replicas are placed randomly by sticking to the per rack replica limit. This can be calculated using the following formula:

(number_of_replicas-1)/number_of_racks+2
-----------------------------------------------

HDFS communication architecture
One of the important aspects of understanding HDFS is taking a look at how different components interact with each other programmatically and 
which type of network protocols or method invocations they use. All communication between HDFS components happens over the TCP/IP protocol.
Different protocol wrappers are defined for different types of communication. The following diagram represents such protocol wrappers, which we 
will look into in this section:



The preceding diagram can be explained as follows:

Client Protocol: This is a communication protocol that's defined for communication between the HDFS Client and the Namenode server. It is
a remote procedure call (RPC) that communicates with NameNode on a defined port using a TCP protocol. All user code and client-side libraries 
that need to interact with Namenode use this protocol. Some of the important methods in this protocol are as follows:
create: Creates a new empty file in the HDFS namespace
append: Appends to the end of file
setReplication: Sets replication of the file
addBlock: Writes additional data blocks to a file and assigns DataNodes for replication
There are multiple methods that are supported by the client protocol. All of these are with respect to client interaction with NameNode. 
We suggest you go over the Hadoop source code on GitHub to understand client and NameNode communication in more detail. This is the location 
of ClientProtocol.java in the Hadoop source code: hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java.
Data Transfer Protocol: The HDFS Client, after receiving metadata information from Namenode, establishes communication with Datanode to read 
and write data. This communication between the client and the Datanode is defined by the Data Transfer Protocol. Since this type of communication
does most of the data heavy lifting in terms of high volume read and writes, it is defined as a streaming protocol and is unlike the RPC protocol 
we defined earlier. Moreover, for efficiency purposes, the client buffers data up to a certain size of HDFS data blocks ( 64 MB by default) and 
then writes one complete block to the respective DataNodes. This protocol is mostly defined around data blocks. Some of the important methods 
that are included in this protocol are as follows:
readBlock: Reads a data block from a DataNode.
writeBlock: Writes a data block to a DataNode.
transferBlock: Transfers a data block from one DataNode to another.
blockChecksum: Gets the checksum value of a data block. This can be an MD5 or a CRC32 value.
The data transfer protocol is an important protocol that defines communication (read/write) between client and data nodes. Additional details 
about the methods it supports can be found in the Hadoop source code on GitHub. The following is the link for 
the same is: https://github.com/lpcclown/hadoop_enhancement/tree/master/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer.

Data Node Protocol: This is another important protocol that you should understand in detail. This protocol defines communication between 
Namenode and DataNode. The Data Node Protocol (DNP) is mostly used by Datanode to provide its operation, health, and storage information to Namenode. 
One of the important aspects of this protocol is that it is a one-way protocol. This means that all requests are always initiated by DataNode.
NameNodes only respond to requests that are initiated by DataNodes. Unlike the previous protocol, this is a RPC protocol that's defined over TCP. 
The following are some of the important methods that are included in this protocol:
registerDatanode: This registers new or rebooted data nodes to NameNode.
sendHeartbeat: This tells NameNode that the DataNode is alive and working properly. This method is not only important from the point of view of
knowing which DataNode is alive, but it also gives NameNode a chance to respond back to DataNodes with a set of commands that it wants them to execute.
For example, at times, NameNode wants to invalidate some of the blocks stored in data nodes. In that case, in response to the sendHeartbeat() method,
NameNode sends an invalidate block request to the DataNode. 
blockReport: This method is used by DataNode to send all of its locally stored block-related information to NameNode. In response to this, 
NameNode sends DataNodes blocks that are obsolete and should be deleted.
----------------------------------------

NameNode internals
HDFS is a distributed File System for storing and managing large datasets. It divides large datasets into small data chunks where each data chunk is
stored on different nodes that are part of the Hadoop cluster. However, HDFS hides these underlying complexities of dividing data into smaller 
chunks and copying that data to different nodes behind abstract file operation APIs. For HDFS users, these file operation APIs are just 
read/write/create/delete operations. All a HDFS user needs to know about is the Hadoop namespace and file URIs. But in reality, lots of steps 
are performed before those operations are completed. One of the key HDFS components in achieving all of these activities is NameNode. NameNode 
in Hadoop is a central component that regulates any operations on HDFS using file metadata that's stored in it. In other words, it manages HDFS
file namespaces. NameNode performs the following functions:

It maintains the metadata of files and directories stored in HDFS. Metadata mostly consists of file creation/modification timestamps, 
access control lists, block or replica storage information, and files' current state.
It regulates any file operations in terms of access control lists stored in files or directories, and which blocks and replicas will be
handled by which DataNode. It also denies an operation if a user is not allowed to perform that operation.
It gives the client information about data blocks and which data node will serve the read/write request.
It also issues commands to DataNodes such as delete corrupted data blocks and also maintains a list of healthy DataNodes. 
NameNodes maintains a data structure called INodes in memory. INodes have all the information regarding files and directories. 
All of these INodes constitute a tree-like structure that is maintained by the Namenodes. INodes contain information such as file or
directory name, username, group name, permissions, authorization ACLs, modification time, access time, and disk space quotas. 
---------------------------------

Data locality and rack awareness
One of the design goals of Hadoop is to move computation to data rather than moving data to computation. This goal was set because Hadoop was 
created for processing high-volume datasets. Moving large datasets can decrease performance. For example, if Hadoop is running MapReduce on a 
large volume of data, it would first try to run mapper tasks on the DataNodes that have the relevant input data (Data Local). This is generally
referred to as data locality optimization in Hadoop. One of the key points to remember here is that reduce tasks do not use data locality, because a
single reduce task can use output from multiple mappers. To achieve data locality, Hadoop uses all three replications (Data Local, Rack Local, and Off rack).
But sometimes, in a very busy cluster, if there are no task slots available on the nodes hosting input data replicas, job schedulers would first try to 
run jobs on the node that have free slots on the same rack (Rack Local). If Hadoop does not find any free slots on the same rack, then tasks are run on 
different racks. However, this will result in data transfer (Off rack). The following diagram shows different types of data locality in Hadoop:



Hadoop understands that any communication between nodes within a rack would be of a lower latency as more network bandwidth is
available within a rack than going outside the rack. Therefore, all components in Hadoop are rack-aware. Rack awareness basically 
means that Hadoop and its components have complete knowledge of the Cluster topology. By cluster topology, we mean how data nodes 
are placed onto different racks that are part of the Hadoop cluster. Hadoop uses this information to ensure data availability in the
case of failures and for better the performance of Hadoop jobs.
-----------------------------------

DataNode internals
HDFS is built based on a master/worker architecture where NameNode is the master and DataNodes are the workers. DataNode follows NameNode's
instructions, such as block creation, replication, and deletion. Read and write requests from clients are served by DataNodes. All the files 
in HDFS are split into blocks and actual data is then stored in the DataNodes. Each DataNode periodically sends its heartbeat to the NameNode 
to acknowledge that it is still alive and functioning properly. DataNodes also send block reports to the NameNodes. 

When the DataNodes receives a new block request, it sends a block received acknowledgement to the NameNode. The Datanode.Java class contains the 
majority of the implementation of Datanode's functionality. This class has the implementation code for communicating with the following:

Client code for read and write operations
DataNode for replication operations
NameNode for block report and heartbeats
The DataNode follows instructions from the NameNode and may delete block replicas or copy replicas to some other DataNodes as per instruction. 
The NameNode does not connect to a DataNode directly for instruction; instead, a client takes metadata information from the NameNode and then 
instructs the DataNode to read/write or copy block replica. An open server socket connection is maintained by the DataNode for client or other 
DataNodes to read and write data. Server information such as host and port number are sent to the NameNode, and the latter sends this information 
to the client when it receives a request for read and write operations.

The NameNode must know which DataNode is functioning properly and thus each DataNode sends a heartbeat to the NameNode at regular intervals. 
The interval is 3 seconds by default. Similarly, block information is also sent to a NameNode by DataNodes at a regular configured interval. 
In short, DataNodes are involved in the following operations:

Heartbeat: All the DataNodes send a regular heartbeat to the NameNode so that the NameNode knows that the DataNodes are working properly and 
can satisfy read/write/delete requests from clients. If the DataNode does not send a heartbeat for a configured period of time, NameNode marks 
he DataNode as dead and does not use that DataNode for any read and write requests. 
Read/write: A DataNode opens a socket connection for clients to read and write data blocks to its storage. The client sends a request to the
NameNode and NameNode replies with a list of DataNodes that can be used for read and write operations. The client then directly uses DataNodes 
to read or write data. 
Replication and block report: During write or replica balance operations, one DataNode may receive a write request for data block write from 
another DataNode. DataNodes also send block reports to the NameNode at regular intervals. This keeps the NameNode up to date regarding the 
location and other details of each block. 

-----------------------------------

Quorum Journal Manager (QJM)
NameNode used to be a single point of failure before the release of Hadoop version 2. In Hadoop 1, each cluster consisted of a single NameNode. 
If this NameNode failed, then the entire cluster would be unavailable. So, until and unless the NameNode service restarted, no one could use the
Hadoop cluster. In Hadoop 2, the high availability feature was introduced. It has two NameNodes, one of the NameNodes is in active state while
the other NameNode is in standby state. The active NameNode serves the client requests while the standby NameNode maintains synchronization of 
its state to take over as the active NameNode if the current active NameNode fails. 

There is a Quorum Journal Manager (QJM) runs in each NameNode. The QJM is responsible for communicating with JournalNodes using RPC; for example, 
sending namespace modifications, that is, edits to JournalNodes, and so on. A JournalNode daemon can run on N machines where N is configurable.
A QJM writes edits to the local disk of a JournalNode running on N machines in the cluster. These JournalNodes are shared with NameNode machines 
and any modifications performed by the active NameNode is logged into edit files on these shared nodes. These files are then read by the standby
NameNode, which applies these modification onto its own fsimage to keep its state in sync with the active NameNode. In case the active NameNode 
fails, the standby NameNode will apply all the changes from the edit logs before changing its state to active and thus making sure that the current 
namespace is fully synchronized. The QJM performs the following operations when it writes to the JournalNode:

The writer makes sure that no other writers are writing to the edit logs. This is to guarantee that even if the two NameNodes are active at a 
same time, only one will be allowed to make namespace changes to the edit logs.
It is possible that the writer has not logged namespace modifications to all the JournalNodes or that some JournalNodes have not completed the 
logging. The QJM makes sure that all the JournalNodes are in sync based on file length.
When one of the preceding two things are verified, the OJM can start a new log segment to write to edit logs.
The writer sends current batch edits to all the JournalNodes in the cluster and waits for an acknowledgement based on the quorum of all the 
JournalNodes before considering the write a success. Those JournalNodes who failed to respond to the write request will be marked as OutOfSync 
and will not be used for the current batch of the edit segment.
A QJM sends a RPC request to JournalNodes to finalize log segmentation. After receiving confirmation from quorum of JournalNodes, QJM can
begin the next log segment. 
The DataNode sends block information and a heartbeat to both the DataNodes to make sure that both have up to date information about the block. 
In Hadoop 3, we can have more than two NameNodes, and the DataNode will send information to all of those NameNodes. In this way, QJM helps in 
achieving high availability.
--------------------------------------

HDFS high availability in Hadoop 3.x
With Hadoop 2.0, active and standby NameNodes were introduced. At any point, out of two NameNodes, one will always be in active state and
other will be in standby state. The active NameNode is the one that's responsible for any client requests in the cluster. Standby NameNodes 
are slave nodes whose responsibility is to keep its state in sync with the active NameNode so that it can provide fast failover in the event
of failover. However, what if one of the NameNodes fails? In that case, the NameNode would become non-HA. This means that NameNodes can only
tolerate up to one failure. This behavior is the opposite of the core fault -tolerant behavior of Hadoop, which certainly can accommodate more
than one failure of DataNodes in a cluster. Keeping that in mind, provisions of more than one standby NameNode was introduced in Hadoop 3. 
The behavior of additional standby NameNodes will still be the same as any other standby NameNode. They will have their own IDs, RPC, and 
HTTP addresses. They will use QJM to get the latest edit logs and update their fsimage.
--------------------------------------------

Data management
We discussed HDFS blocks and replication in the previous sections. NameNode stores all metadata information and is a single point of failure, which means
that no one can use HDFS if NameNode is down. This metadata information is important and can be used to restart NameNode on other machines. 
Thus, it is important to take multiple backup copies of a metadata file so that, even if metadata is lost from the primary NameNode, the backup 
copy can be used to restart the NameNode on the same machine or another machine. In this section, we will discuss NameNode metadata files such
as fsimage and edit log. We will discuss data integrity further by using checksum and taking snapshots of the directory to avoid data loss and modification. 
------------------------------------
Metadata management
HDFS stores a large amount of structured and unstructured data in various formats. While the data is continuously growing to terabytes and petabytes, 
and your data is being used by Hadoop, you are likely to come across questions, such as what data is available on HDFS, how it is being used, and what
type of users are using the data, the data creation timeline, and so on. Well-maintained metadata information can effectively answer these questions and 
thus improve the usability of the data store over HDFS.

NameNode keeps the complete fsimage in memory so that all the metadata information requests can be served in the smallest amount of time possible and 
persist fsimage and edit logs on the disk. fsimage contains HDFS directory information, file information, permissions, quotas, last access times and 
last modification times, and block IDs for files.

HDFS metadata information includes various attributes of directories and files, such as ownership, permissions, quotas, replication factors, and much more. 
This information is in two files:

fsimage: An fsimage file contains the complete state of the File System to which every File System modification is assigned a unique and an unmodulated 
increasing transaction ID. An fsimage file represents the File System state up to a specific transaction ID.

edits: An edits log file contains a list of changes that are applied on the File System after the most recent fsimage. The edit log contains an 
entry for each operation and the checkpoint operation periodically merges fsimage and the edit log by applying all of the changes that are available 
in the edit logs in fsimage before saving the new fsimage.
----------------------------------------------

Checkpoint using a secondary NameNode
Checkpoint is the process of merging an fsimage with edit logs by applying all the actions of the edit log on the fsimage. This process is necessary
to make sure that the edit log does not grow too large. Let's go into further details of how the checkpoint process works in Hadoop.

In the previous section, we discussed the fsimage and the edit log file. NameNode loads the fsimage into memory when it starts and then applies
edits from the edit log file to the fsimage. Once this process is complete, it then writes a new fsimage file to the system. At the end of this
operation, the edit log file does not have anything in it. This process starts only during NameNode startup—it does not perform merging operations 
while the NameNode is live and is busy serving a request. If the NameNode is up for a long period of time, then the edit log file could get too big.
Therefore, we may need to have a service that periodically merges edit logs and the fsimage file. The Secondary NameNode does the job of merging the 
fsimage and the edit log file. The interval for the checkpoint operation and the number of transactions  in the edit log is controlled by 
two configuration parameters: dfs.namenode.checkpoint.period for intervals and dfs.namenode.checkpoint.txns for the number of transactions. 
This means that if the limit is reached, then the checkpoint process will be forcefully started, even if the interval period has not been reached. 
The secondary NameNode also stores the latest fsimage file so that it can be used if anything is required.
------------------------------------------

Data integrity
Data integrity ensures that no data is lost or corrupted during storage or processing of data. HDFS stores huge volumes of data that consists of a
large number of HDFS blocks. Typically, in a big cluster that consist of thousands of nodes, there are more chances of machine failures. Imagine 
that your replication factor is three and two of the machines storing replication for a particular block failed and that the last replica block is
corrupted. You may lose your data in such cases, and so it is necessary to configure a good replication factor and do regular block scanning to
verify that the block is not corrupted. HDFS maintains data integrity using the checksum mechanism.

Checksum: Checksum is calculated for each block that is written to HDFS. HDFS maintains checksum for each block and verifies checksum when it reads 
data. The DataNode is responsible for storing data and checksum is responsible for all of the data stored on it. In this way, when the client reads 
data from the DataNode, they also read the checksum of data. The DataNode regularly runs a block scanner to verify the data blocks stored on them. 
If a corrupt block is found, HDFS reads a replica of the corrupted block and replaces the block with a new replica. Let's see how checksum verification
happens in read and write operations:

HDFS write: The DataNode is responsible for verifying the checksum of the block. During the write operation, the checksum is created for a file that
has to be written to HDFS. Previously, we discussed the HDFS write operation, where a file is split into blocks and HDFS creates a block pipeline.
The DataNode, which is responsible for storing the last block in the pipeline, compares the checksum, and if the checksum does not match, it sends 
ChecksumException to the client and the client can then take necessary action, such as retrying the operation and so on. 
HDFS read: When the clients starts reading data from the DataNode, it also compares the checksum of the block and if the checksum is not equal, then 
it sends information to the NameNode so that the NameNode marks the block as corrupted and takes necessary action to replace the corrupted block 
with another replica. The NameNode does not use these DataNodes for any other client request until it is replaced or removed from the corrupted entry list. 
------------------------------------------

Data rebalancing
HDFS is a scalable distributed storage File System, and data stored on it increases over time. As data volume increases, a few DataNodes may hold more 
blocks than other DataNodes, and this may cause more read and write requests to DataNodes with more blocks. Thus, these DataNodes will be very busy 
serving requests compared to other DataNodes.

HDFS is a scalable system and consists of commodity hardware. On large clusters where data volumes are high, the chances of DataNodes failing is higher. 
Also, adding new DataNodes to manage data volume is common. The removal or addition of DataNodes can cause data to be skewed where a few DataNodes 
hold more blocks than others. To avoid such problems, HDFS has a tool known as a balancer. Let's see how HDFS stores blocks and what scenario it 
takes into consideration.

When a new request comes to store a block, HDFS considers the following approaches:

Distributing data uniformly across the DataNode in a cluster.
Storing one replica on the same rack where the first block was written. This helps optimize cross-rack I/O.
Storing another replica on a different rack to support fault-tolerance in the case of rack failure.
When a new node is added, HDFS does not distribute previously stored blocks to it. Instead, it uses this DataNode to store new blocks. 
If a failed DataNode is removed, then a few of the blocks will be under-replicated. Therefore, HDFS will balance the replicas by storing them in different DataNodes. 
The balancer is used to balance the HDFS cluster, and by doing so it evenly distributes blocks across all the DataNodes in a cluster. Now, the 
obvious question is, when do we call a cluster a balance cluster? We can call a cluster a balanced cluster when the percentage of free space or 
utilized space of all DataNodes is above or below the specified threshold size. The balancer maintains the threshold by moving data from 
over-utilized DataNodes to under-utilized DataNodes, which ensures that all DataNodes have an equal amount of free space.


Let's look at two important properties that are used with the balancer:

Threshold: The threshold is used to ensure that the overall usage of all DataNodes does not exceed or drop lower than the configured threshold
percentage for the overall cluster usage. Simply, if the overall cluster usage is 60% and the threshold that's been configured is 5%, then each 
DataNode usage capacity should be between 55% to 65%. The default threshold is 10%, but you can change it by using the following command when you 
are running the balancer:
        $ hdfs balancer –threshold 15
In the preceding case, if the overall disk usage is 60% and we run the balancer using the preceding command, then the balancer will make sure that 
the cluster usage at each DataNode is between 45% and 75%. This means that the balancer will only balance those DataNodes whose usage percentage is 
less than 45% and more then 75%. 

Policy: Policy is of two types: one is DataNode and the other is Blockpool. By default, the value is used to balance storage at the DataNode level, but 
for clusters where we are using the HDFS Federation service, we should change it to a Blockpool so that balancer ensures that blocks from one Blockpool 
do not move to another. 
The primary objective of balancer is to move data from the DataNode whose threshold is higher than DFS usage to a DataNode whose threshold is 
lower than the DFS usage. The balancer also takes care of the rack policy, where it minimizes the data transfer between two different racks.
----------------------------

Best practices for using balancer 
In this section, we will talk about how we can optimize a balancer job, when to use balancer, and some best practices regarding it.

We should always run balancer when a new node is added to a cluster because the newly added node will have no block initially and it will be under-utilized. 
Normally, in a big cluster that consists of a large number of DataNode servers, it is good practice to run balancer at regular intervals. The idea is to a
schedule one job, which will take care of running the balancer at regular intervals. Don't worry if the balancer is already running and the cron job has 
scheduled another balancing job—the new balancer will not start until the previous ends its execution.

Balancer is also a task and it must finish as early as possible. Each DataNode allocates 10 MBPS bandwidth for the balancer job. We may want to take 
care of two things: allocating more bandwidth to the DataNode should not affect other jobs, and getting the maximum performance from the balancer by 
increasing the bandwidth. Generally, if you have 200 MBPS bandwidth, you can allocate 10% of it, that is, 20 MBPS for the balancer without impacting on
other jobs. You can use the following command to increase the bandwidth to 15 MBPS:

$ su hdfs -c 'hdfs dfsadmin -setBalancerBandwidth 15728640'
It is good practice to invoke the balancer when the cluster is not using its resources extensively. In such a case, it is easy to ask for more 
bandwidth for the balancer and the balancer will finish earlier than expected. 
---------------------------

HDFS reads and writes
HDFS is a Distributed File Storage system in which we write data and then read that same data. NameNode is a master node that contains metadata information
about all the files and DataNode space. The DataNode is a worker node that stores real files. Every read and write request will go through NameNode. HDFS
is known for the write once read many pattern, meaning that the file is written in HDFS only once and can be read many times. Files stored in HDFS are not
editable but appending new data to a file is allowed. 

In this section, we will cover the internals of HDFS read and write operations, and will see how clients communicate with NameNode and DataNode for 
read/write operations.
----------------------------

Write workflows
HDFS provides us with the capability to write, read, and delete files from its storage system. You can write data using the command-line utility or by 
using the programming API interface. However, the write workflow remains the same in both cases. We will go through the internals of HDFS write in this
section. The following diagram gives a high-level overview of HDFS write: 



To write a file to HDFS, the HDFS client uses the DistributedFileSystem API and calls its create() method. The signature of the create method is 
given as follows, which is a part of the FileSystem class. The File System is the parent class of DistributedFileSystem:

public FSDataOutputStream create(Path f) throws IOException {
  return create(f, true);
}

public FSDataOutputStream create(Path f, boolean overwrite)
    throws IOException {
  return create(f, overwrite, 
                getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
                    IO_FILE_BUFFER_SIZE_DEFAULT),
                getDefaultReplication(f),
                getDefaultBlockSize(f));
}
 The methods that are mentioned in the preceding code are abstract methods whose implementation is in DistsibutedFileSystem:

public abstract FSDataOutputStream create(Path f,
    FsPermission permission,
    boolean overwrite,
    int bufferSize,
    short replication,
    long blockSize,
    Progressable progress) throws IOException;
DistributedFileSystem makes an RPC call to NameNode by creating a new file. NameNode checks whether the file exists or not. If it already exists, it will
throw IOException with a message stating that the file already exists. If the file does not exist, NameNode then uses FSPermission to check whether the
user has permission to write the file to the mentioned location. Once the permission check is successful, the NameNode makes a record of the new file. 
If it isn't successful, it will return IOException with a message stating that permission was denied. 

The return type of create() is FSDataOutputStream, which is written to that client after the successful execution of create(). The client uses 
FsDataOutputStream and we call the write method to write data.

DFSOutputStream is responsible for splitting data into packets of block size. Data is written to an internal data queue called DFSPacket , which contains 
data, its checksum, sequence number, and other information.

DataStreamer contains a Linked List of DFSPacket, and for each packet it asks NameNode for new DataNodes to store packets and their replicas. 
DataNodes returned by NameNode form a pipeline, and DataStreamer writes a packet to the first DataNode in the pipeline. The first DataNode stores 
the packet and forwards it to the second DataNode in the pipeline. This process is repeated until the last DataNode in the pipeline stores the packet. 
The number of DataNodes in the pipeline depends on the replication factor that's been configured. All of the data blocks are stored in parallel. 

DFSOutputStream also maintains an acknowledgement queue (Linked List) of packets for which acknowledgement is not received from the DataNode. Once 
the packet is copied by the DataNode in the pipeline, the DataNode sends an acknowledgement. This acknowledgement queue is used to restart the 
operation if the data node in the pipeline fails.

Once the HDFS client finishes writing data, it closes the stream by calling close() on the stream. The close operation flushes the remaining data 
to the pipeline and then waits for an acknowledgement.

Finally, the client sends a completion signal to the NameNode after its final acknowledgement is received. Thus, NameNode has information about
all of the packets and their block location, which can be accessed while reading the file. 
--------------------------------
Read workflows
We have seen how a file can be written to HDFS and how HDFS works internally to make sure that the file is written in a distributed fashion. Now, we will
see how a file is read using the HDFS client and how it works internally. Similar to HDFS write, NameNode is also a primary contact for the read operation.
The following diagram shows the detailed steps of the file read operation of HDFS:



HDFS calls open() on a particular file that it wants to use by using the FileSystem object, which internally calls open() of DistributedFileSystem: 

public FSDataInputStream open(Path f) throws IOException {
  return open(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
      IO_FILE_BUFFER_SIZE_DEFAULT));
} 

public abstract FSDataInputStream open(Path f, int bufferSize)
  throws IOException;
NameNode returns IOException with the appropriate message the client does not have permission to read the file or the file does not exist.

NameNode contains all the metadata information about the files. DistributedFileSystem makes an RPC call to the NameNode to get blocks of files. NameNode
returns a list of DataNodes for each block, which are sorted based on the proximity of the HDFS client, that is, the NameNode nearest to the client will 
be first in the list. 

The open() method returns FSDataInputStream to the client to read data. DFSInputStream is wrapped within FSDataInputStream and is responsible for managing 
the DataNodes. DFSInputStream connects to the DataNode using the DataNode addresses, which it received in the first block. Data is returned to the client 
in the form of a stream.

When data from the block is read successfully, DFSInputStream will close the connection with the DataNode and then take the nearest DataNode for the
next block of the file. Data is then streamed from the DataNode back to the client to which the client calls the read() method repeatedly on 
the stream. When the block ends, DFSInputStream closes the connection to the DataNode to which the DFSInputStream then finds the
suitable DataNode for the next block.

It's obvious that the DataNode may fail or return an error to DFSInputStream. In the case of an error or failure, DFSInpurStream makes an entry of 
failed DataNodes so that it does not connect to these DataNodes for the next blocks and then connects to the next closest DataNode that contains a 
replica for the block. Then, it reads the data from there. 
DFSInputStream also verifies the checksum of the block and if it does not match and finds that the block is corrupted, it reports it to NameNode and 
then selects the next closest DataNode that contains a replica of the block to read data.

Once the client has finished reading data from all the blocks, it closes the connection using close() on the stream. 

Every operation request will go through the NameNode, and the NameNode helps the client by providing metadata information about the request.
-----------------------------------------------------
