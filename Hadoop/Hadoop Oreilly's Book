The version 2 release made significant leaps compared to version 1 of Hadoop. It introduced YARN, a sophisticated general-purpose
resource manager and job scheduling component. HDFS high availability, HDFS federations, and HDFS snapshots were some other prominent features introduced 
in version 2 releases.

The core idea behind the MapReduce model was to provide parallelism, fault tolerance, and data locality features. Data locality means a program is 
executed where data is stored instead of bringing the data to the program.

NameNodes and DataNodes have a specific role in managing overall clusters. NameNodes are responsible for maintaining metadata information.
MapReduce engines have a job tracker and task tracker whose scalability is limited to 40,000 nodes because the overall work of scheduling and 
tracking is handled by only the job tracker. YARN was introduced in Hadoop version 2 to overcome scalability issues and resource management jobs. 
It gave Hadoop a new lease of life and Hadoop became a more robust, faster, and more scalable system.

---------------------------------------------------------
Overview of Hadoop 3 and its features
The first alpha release of Hadoop version 3.0.0 was on 30 August 2016. It was called version 3.0.0-alpha1. This was the first alpha release in a series of planned
alphas and betas that ultimately led to 3.0.0 GA. The intention behind this alpha release was to quickly gather and act on feedback from downstream users.

With any such releases, there are some key drivers that lead to its birth. These key drivers create benefits that will ultimately help in the better functioning 
of Hadoop-augmented enterprise applications. Before we discuss the features of Hadoop 3, you should understand these driving factors. Some driving factors behind 
the release of Hadoop 3 are as follows:

A lot of bug fixes and performance improvements: Hadoop has a growing open source community of developers regularly adding major/minor changes or improvements to 
the Hadoop trunk repository. These changes were growing day by day and they couldn't be accommodated in minor version releases of 2.x. They had to be accommodated 
with a major version release. Hence, it was decided to release the majority of these changes committed to the trunk repository with Hadoop 3.
Overhead due to data replication factor: As you may be aware, HDFS has a default replication factor of 3. This helps make things more fault-tolerant with better 
data locality and better load balancing of jobs among DataNodes. However, it comes with an overhead cost of around 200%. For non-frequently accessed datasets 
that have low I/O activities, these replicated blocks are never accessed in the course of normal operations. On the other hand, they consume the same number of 
resources as other main resources. To mitigate this overhead with non-frequently accessed data, Hadoop 3 introduced a major feature, called erasure coding. 
This stores data durably while saving space significantly.
Improving existing YARN Timeline services: YARN Timeline service version 1 has limitations that impact reliability, performance, and scalability. For example, 
it uses local-disk-based LevelDB storage that cannot scale to a high number of requests. Moreover, the Timeline server is a single point of failure. To mitigate 
such drawbacks, YARN Timeline server has been re-architected with the Hadoop 3 release.
Optimizing map output collector: It is a well-known fact that native code (written correctly) is faster to execute. In lieu of that, some optimization is done 
in Hadoop 3 that will speed up mapper tasks by approximately two to three times. The native implementation of map output collector has been added, which will
be used in the Java-based MapReduce framework using the Java Native Interface (JNI). This is particularly useful for shuffle-intensive operations.
The need for a higher availability factor of NameNode: Hadoop is a fault-tolerant platform with support for handling multiple data node failures. In the case
of NameNodes versions, prior to Hadoop version 3 support two NameNodes, Active and Standby. While it is a highly available solution, in the case of the failure 
of an active (or standby) NameNode, it will go back to a non-HA mode. This is not very accommodative of a high number of failures. In Hadoop 3, support for more
than one standby NameNode has been introduced.
Dependency on Linux ephemeral port range: Linux ephemeral ports are short-lived ports created by the OS (operating system) when a process requests any available port.
The OS assigns the port number from a predefined range. It then releases the port after the related connection terminates. With version 2 and earlier, many Hadoop 
services' default ports were in the Linux ephemeral port range. This means starting these services sometimes failed to bind to the port due to conflicts with 
other processes. In Hadoop 3, these default ports are moved out of the ephemeral port range.
Disk-level data skew: There are multiple disks (or drives) managed by DataNodes. Sometimes, adding or replacing disks leads to significant data skew within 
a DataNode. To rebalance data among disks within a DataNode, Hadoop 3 has introduced a CLI utility called hdfs diskbalancer.
Well! Hopefully, by now, you have a clear understanding of why certain features were introduced in Hadoop 3 and what kinds of benefits are derived from them. 

-------------------------------------------------

Defining HDFS
HDFS is designed to run on a cluster of commodity hardware. It is a fault-tolerant, scalable File System that handles the failure of nodes without
data and can scale up horizontally to any number of nodes. The initial goal of HDFS was to serve large data files with high read and write performance.

The following are a few essential features of HDFS:

Fault tolerance: Downtime due to machine failure or data loss could result in a huge loss to a company; therefore, the companies want a highly available 
fault-tolerant system. HDFS is designed to handle failures and ensures data availability with corrective and preventive actions.
Files stored in HDFS are split into small chunks and each chunk is referred to as a block. Each block is either 64 MB or 128 MB, depending on the configuration. 
Blocks are replicated across clusters based on the replication factor. This means that if the replication factor is three, then the block will be replicated to
three machines. This assures that, if a machine holding one block fails, the data can be served from another machine.

Streaming data access: HDFS works on a write once read many principle. Data within a file can be accessed by an HDFS client. Data is served in the form of streams, 
which means HDFS enables streaming access to large data files where data is transferred as a continuous stream. HDFS does not wait for the entire file to be 
read before sending data to the client; instead, it sends data as soon as it reads it. The client can immediately process the received stream, which makes 
data processing efficient.

Scalability: HDFS is a highly scalable File System that is designed to store a large number of big files and allows you to add any number of machines to 
increase its storage capability. Storing a huge number of small files is generally not recommended; the size of the file should be equal to or greater than 
the block size. Small files consume more RAM space on master nodes, which may decrease the performance of HDFS operations.

Simplicity: HDFS is easy to set up and manage. It is written in Java. It provides easy command-line interface tools that are very much similar to Linux commands. 
Later in this chapter, we will see how easy it is to operate HDFS via a command-line utility. 

High availability: HDFS is a highly available distributed File System. Every read and write request goes to a master node, and a master node can be a 
single point of failure. Hadoop offers the high availability feature, which means a read and write request will not be affected by the failure of the active 
master node. When the active master node fails, the standby master node takes over. In Hadoop version 3, we can have more than two master nodes running at 
once to make high availability more robust and efficient.
-------------------------------------------------

Deep dive into the HDFS architecture
As a big data practitioner or enthusiast, you must have read or heard about the HDFS architecture. The goal of this section is to explore the 
architecture in depth, including the main and essential supporting components. By the end of this section, you will have a deep knowledge of 
the HDFS architecture, along with the intra-process communication of architecture components. But first, let's start by establishing definition 
of HDFS (Hadoop Distributed File System). HDFS is the storage system of the Hadoop platform, which is distributed, fault-tolerant, 
and immutable in nature. HDFS is specifically developed for large datasets (too large to fit in cheaper commodity machines). 
Since HDFS is designed for large datasets on commodity hardware, it purposely mitigates some of the bottlenecks associated with large datasets.

We will understand some of these bottlenecks and how HDFS mitigates them here:

With large datasets comes the problem of slow processing they are run on only one computer. The Hadoop platform consists of two 
logical components, distributed storage and distributed processing. HDFS provides distributed storage. MapReduce and other YARN-compatible 
frameworks provide distributed processing capabilities. To mitigate this, Hadoop offers distributed data processing, which has several 
systems processing a chunk of data simultaneously. 

With distributed processing of large datasets, one of the challenges is to mitigate large data movements over the network. HDFS makes
 provisions for applications or code to move the computation closer to where the data is located. This ensures less utilization of cluster 
 bandwidth. Moreover, data in HDFS is replicated by default and each replica is hosted by a different node. This replication helps in moving
 computation closer to the data. For example, if the node hosting one of the replicas of the HDFS block is busy and does not have any open 
 slots for running jobs, then the computation would be moved to another node hosting some other HDFS block replica.
 
With large datasets, the cost of failure is greater. So, if a complex process on large datasets (running over a longer duration) fails, 
then rerunning that complex data processing job is significant in terms of resource costs and time consumption. Moreover, one of the side 
effects of distributed processing is that the chances of failure are high due to high network communication and coordination across a large 
number of machines. Lastly, it runs on commodity hardware, where failure is unavoidable. To mitigate such risks, HDFS is built with an automated 
mechanism to detect and recover from faults.

HDFS is designed to be a File System that is used for multiple access by end users and processes in a distributed cluster. In the case of 
multiple random access, having provisions for modifying files at arbitrary positions is error-prone and difficult to manage. To mitigate this 
risk, HDFS is designed to support a simple coherency model where the file is not allowed to be modified at arbitrary points once it has been 
written, created, and closed for the first time. You can only add content at the end of a file or truncate it completely. This simple 
coherency model keeps the HDFS design simple, scalable, and less buggy. 
---------------------------------------------------

