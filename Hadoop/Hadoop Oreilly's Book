The version 2 release made significant leaps compared to version 1 of Hadoop. It introduced YARN, a sophisticated general-purpose
resource manager and job scheduling component. HDFS high availability, HDFS federations, and HDFS snapshots were some other prominent features introduced 
in version 2 releases.

The core idea behind the MapReduce model was to provide parallelism, fault tolerance, and data locality features. Data locality means a program is 
executed where data is stored instead of bringing the data to the program.

NameNodes and DataNodes have a specific role in managing overall clusters. NameNodes are responsible for maintaining metadata information.
MapReduce engines have a job tracker and task tracker whose scalability is limited to 40,000 nodes because the overall work of scheduling and 
tracking is handled by only the job tracker. YARN was introduced in Hadoop version 2 to overcome scalability issues and resource management jobs. 
It gave Hadoop a new lease of life and Hadoop became a more robust, faster, and more scalable system.

---------------------------------------------------------
Overview of Hadoop 3 and its features
The first alpha release of Hadoop version 3.0.0 was on 30 August 2016. It was called version 3.0.0-alpha1. This was the first alpha release in a series of planned
alphas and betas that ultimately led to 3.0.0 GA. The intention behind this alpha release was to quickly gather and act on feedback from downstream users.

With any such releases, there are some key drivers that lead to its birth. These key drivers create benefits that will ultimately help in the better functioning 
of Hadoop-augmented enterprise applications. Before we discuss the features of Hadoop 3, you should understand these driving factors. Some driving factors behind 
the release of Hadoop 3 are as follows:

A lot of bug fixes and performance improvements: Hadoop has a growing open source community of developers regularly adding major/minor changes or improvements to 
the Hadoop trunk repository. These changes were growing day by day and they couldn't be accommodated in minor version releases of 2.x. They had to be accommodated 
with a major version release. Hence, it was decided to release the majority of these changes committed to the trunk repository with Hadoop 3.
Overhead due to data replication factor: As you may be aware, HDFS has a default replication factor of 3. This helps make things more fault-tolerant with better 
data locality and better load balancing of jobs among DataNodes. However, it comes with an overhead cost of around 200%. For non-frequently accessed datasets 
that have low I/O activities, these replicated blocks are never accessed in the course of normal operations. On the other hand, they consume the same number of 
resources as other main resources. To mitigate this overhead with non-frequently accessed data, Hadoop 3 introduced a major feature, called erasure coding. 
This stores data durably while saving space significantly.
Improving existing YARN Timeline services: YARN Timeline service version 1 has limitations that impact reliability, performance, and scalability. For example, 
it uses local-disk-based LevelDB storage that cannot scale to a high number of requests. Moreover, the Timeline server is a single point of failure. To mitigate 
such drawbacks, YARN Timeline server has been re-architected with the Hadoop 3 release.
Optimizing map output collector: It is a well-known fact that native code (written correctly) is faster to execute. In lieu of that, some optimization is done 
in Hadoop 3 that will speed up mapper tasks by approximately two to three times. The native implementation of map output collector has been added, which will
be used in the Java-based MapReduce framework using the Java Native Interface (JNI). This is particularly useful for shuffle-intensive operations.
The need for a higher availability factor of NameNode: Hadoop is a fault-tolerant platform with support for handling multiple data node failures. In the case
of NameNodes versions, prior to Hadoop version 3 support two NameNodes, Active and Standby. While it is a highly available solution, in the case of the failure 
of an active (or standby) NameNode, it will go back to a non-HA mode. This is not very accommodative of a high number of failures. In Hadoop 3, support for more
than one standby NameNode has been introduced.
Dependency on Linux ephemeral port range: Linux ephemeral ports are short-lived ports created by the OS (operating system) when a process requests any available port.
The OS assigns the port number from a predefined range. It then releases the port after the related connection terminates. With version 2 and earlier, many Hadoop 
services' default ports were in the Linux ephemeral port range. This means starting these services sometimes failed to bind to the port due to conflicts with 
other processes. In Hadoop 3, these default ports are moved out of the ephemeral port range.
Disk-level data skew: There are multiple disks (or drives) managed by DataNodes. Sometimes, adding or replacing disks leads to significant data skew within 
a DataNode. To rebalance data among disks within a DataNode, Hadoop 3 has introduced a CLI utility called hdfs diskbalancer.
Well! Hopefully, by now, you have a clear understanding of why certain features were introduced in Hadoop 3 and what kinds of benefits are derived from them. 

-------------------------------------------------

Defining HDFS
HDFS is designed to run on a cluster of commodity hardware. It is a fault-tolerant, scalable File System that handles the failure of nodes without
data and can scale up horizontally to any number of nodes. The initial goal of HDFS was to serve large data files with high read and write performance.

The following are a few essential features of HDFS:

Fault tolerance: Downtime due to machine failure or data loss could result in a huge loss to a company; therefore, the companies want a highly available 
fault-tolerant system. HDFS is designed to handle failures and ensures data availability with corrective and preventive actions.
Files stored in HDFS are split into small chunks and each chunk is referred to as a block. Each block is either 64 MB or 128 MB, depending on the configuration. 
Blocks are replicated across clusters based on the replication factor. This means that if the replication factor is three, then the block will be replicated to
three machines. This assures that, if a machine holding one block fails, the data can be served from another machine.

Streaming data access: HDFS works on a write once read many principle. Data within a file can be accessed by an HDFS client. Data is served in the form of streams, 
which means HDFS enables streaming access to large data files where data is transferred as a continuous stream. HDFS does not wait for the entire file to be 
read before sending data to the client; instead, it sends data as soon as it reads it. The client can immediately process the received stream, which makes 
data processing efficient.

Scalability: HDFS is a highly scalable File System that is designed to store a large number of big files and allows you to add any number of machines to 
increase its storage capability. Storing a huge number of small files is generally not recommended; the size of the file should be equal to or greater than 
the block size. Small files consume more RAM space on master nodes, which may decrease the performance of HDFS operations.

Simplicity: HDFS is easy to set up and manage. It is written in Java. It provides easy command-line interface tools that are very much similar to Linux commands. 
Later in this chapter, we will see how easy it is to operate HDFS via a command-line utility. 

High availability: HDFS is a highly available distributed File System. Every read and write request goes to a master node, and a master node can be a 
single point of failure. Hadoop offers the high availability feature, which means a read and write request will not be affected by the failure of the active 
master node. When the active master node fails, the standby master node takes over. In Hadoop version 3, we can have more than two master nodes running at 
once to make high availability more robust and efficient.
-------------------------------------------------

Deep dive into the HDFS architecture
As a big data practitioner or enthusiast, you must have read or heard about the HDFS architecture. The goal of this section is to explore the 
architecture in depth, including the main and essential supporting components. By the end of this section, you will have a deep knowledge of 
the HDFS architecture, along with the intra-process communication of architecture components. But first, let's start by establishing definition 
of HDFS (Hadoop Distributed File System). HDFS is the storage system of the Hadoop platform, which is distributed, fault-tolerant, 
and immutable in nature. HDFS is specifically developed for large datasets (too large to fit in cheaper commodity machines). 
Since HDFS is designed for large datasets on commodity hardware, it purposely mitigates some of the bottlenecks associated with large datasets.

We will understand some of these bottlenecks and how HDFS mitigates them here:

With large datasets comes the problem of slow processing they are run on only one computer. The Hadoop platform consists of two 
logical components, distributed storage and distributed processing. HDFS provides distributed storage. MapReduce and other YARN-compatible 
frameworks provide distributed processing capabilities. To mitigate this, Hadoop offers distributed data processing, which has several 
systems processing a chunk of data simultaneously. 

With distributed processing of large datasets, one of the challenges is to mitigate large data movements over the network. HDFS makes
 provisions for applications or code to move the computation closer to where the data is located. This ensures less utilization of cluster 
 bandwidth. Moreover, data in HDFS is replicated by default and each replica is hosted by a different node. This replication helps in moving
 computation closer to the data. For example, if the node hosting one of the replicas of the HDFS block is busy and does not have any open 
 slots for running jobs, then the computation would be moved to another node hosting some other HDFS block replica.
 
With large datasets, the cost of failure is greater. So, if a complex process on large datasets (running over a longer duration) fails, 
then rerunning that complex data processing job is significant in terms of resource costs and time consumption. Moreover, one of the side 
effects of distributed processing is that the chances of failure are high due to high network communication and coordination across a large 
number of machines. Lastly, it runs on commodity hardware, where failure is unavoidable. To mitigate such risks, HDFS is built with an automated 
mechanism to detect and recover from faults.

HDFS is designed to be a File System that is used for multiple access by end users and processes in a distributed cluster. In the case of 
multiple random access, having provisions for modifying files at arbitrary positions is error-prone and difficult to manage. To mitigate this 
risk, HDFS is designed to support a simple coherency model where the file is not allowed to be modified at arbitrary points once it has been 
written, created, and closed for the first time. You can only add content at the end of a file or truncate it completely. This simple 
coherency model keeps the HDFS design simple, scalable, and less buggy. 
---------------------------------------------------

HDFS logical architecture
We'll now gain an understanding of some of the design decisions of HDFS and how they mitigate some of the bottlenecks associated 
with a large dataset's storage and processing in a distributed manner. It's time to take a deep dive into the HDFS architecture. 
The following diagram represents the logical components of HDFS:




For simplicity's sake, you can divide the architecture into two groups. One group can be called the data group. 
It consists of processes/components that are related to file storage. The other group can be called the management group. 
It consists of processes/components that are used to manage data operations such as read, write, truncate, and delete.

So, the data group is about data blocks, replication, checkpoints, and file metadata. The management group is about NameNodes, 
DataNodes, JournalNodes, and Zookeepers. We will first take at the management group's components and then we will talk about the data group's components:

NameNode: HDFS is a master-slave architecture. The NameNode plays the role of a master in the HDFS architecture. It is the regulator that controls 
all operations on the data and stores all relevant metadata about data that's stored in HDFS. All data operations will first go through a NameNode and 
then to other relevant Hadoop components. The NameNode manages the File System namespace. It stores the File System tree and metadata of files and 
directories. All of this information is stored on the local disk in three types of files, namely File System namespace, image (fsimage) files, and edit logs files.

The fsimage file stores the state of the File System at a point in time. The edit logs files contains a list of all changes (creation, modification, 
truncation, or deletion) that are made to each HDFS file after the last fsimage file was created. A new fsimage file is created after collaborating 
the content of the most recent fsimage files with the latest edit logs. This process of merging fsimage files with edit logs is called checkpointing.
 It is system-triggered and is managed by system policies. NameNode also maintains a mapping of all data blocks to DataNode.
 
DataNode: DataNodes plays the role of slaves in the HDFS architecture. They perform data block operations (creation, modification, or deletion)
 based on instructions that are received from NameNodes or HDFS clients. They host data processing jobs such as MapReduce. They report back 
 block information to NameNodes. DataNodes also communicate between each other in the case of data replication.
 
JournalNode: With NameNode high availability, there was a need to manage edit logs and HDFS metadata between a active and standby NameNodes. 
JournalNodes were introduced to efficiently share edit logs and metadata between two NameNodes. JournalNodes exercise concurrency write locks 
to ensure that edit logs are written by one active NameNode at a time. This level of concurrency control is required to avoid the state of a 
NameNode from being managed by two different services that act as failovers of one another at the same time. This type of scenario, where edit
 logs are managed by two services at the same time, is called HDFS split brain scenario, and it can result in data loss or inconsistent state. 
 JournalNodes avoid such scenarios by allowing only one NameNode to be writing to edit logs at a time.
 
Zookeeper failover controllers: With the introduction of high availability (HA) in NameNodes, automatic failover was introduced as well. 
HA without automatic failover would have manual intervention to bring NameNode services back up in the event of failure. This is not ideal. 
Hence, the Hadoop community has introduced two components: Zookeeper Quorum and Zookeeper Failover controller, also known as ZKFailoverController (ZKFC). 
Zookeeper maintains data about NameNode health and connectivity. It monitors clients and notifies other clients in the event of failure. 
Zookeeper maintains an active persistent session with each of the NameNodes, and this session is renewed by each of them upon expiry. 
In the event of a failure or crash, the expired session is not renewed by the failed NameNode. This is when Zookeeper informs other 
standby NameNodes to initiate the failover process. Every NameNode server has a Zookeeper client installed on it. This Zookeeper 
client is called ZKFC. Its prime responsibilities are monitoring the health of the NameNode processes, managing sessions with Zookeeper servers, 
and acquiring write lock concurrency in the case of its local NameNode being active. ZKFC monitors NameNode health with periodic health check pings. 
If a NameNode responds to those pings in a timely manner, then ZKFC considers that NameNode to be healthy. If not, then ZKFC considers it to 
be unhealthy and accordingly notifies the Zookeeper servers about it. In the case of the local NameNode being healthy and active, ZKFC opens 
a session in Zookeeper and creates a lock znode on the Zookeeper servers. This znode is ephemeral in nature and will be deleted
 automatically when the session expires.
 --------------------------------------------------
 
 Blocks
Blocks define the minimum amount of data that HDFS can read and write at a time. HDFS, when storing large files, divides them into sets of individual 
blocks and stores each of these blocks on different data nodes in a Hadoop cluster. All files are divided into data blocks and then stored in HDFS. 
The default value of a HDFS block size is either 64 MB or 128 MB. This is large compared to Unix-level File System blocks. Having a large HDFS data 
block size is beneficial in the case of storing and processing large volumes of data in Hadoop. One of the reasons for this is to efficiently manage
the metadata associated with each data block. If the size of the data blocks are too small, then more metadata will be stored in NameNodes, causing 
its RAM to be filled up quickly. This will also result in more remote procedural calls (RPCs) to NameNode ports, which may result in resource contention.

The other reason is that large data blocks would result in higher Hadoop throughput. With an appropriate data block size, you can strike a balance 
between how many data nodes would be running parallel processes to perform operations on a given dataset and how much data can be processed by an 
individual process given the amount of resources allocated to it. Larger data blocks also result in less time being spent in disk-seeking operations
or finding out the start of the data block. In addition to the advantages of having a large HDFS block size, the concept of HDFS block abstraction 
have other advantages in Hadoop operations. One such benefit is that you can store files larger than the size of a disk of an individual machine.
The other benefit is that it provides better replication strategy and failover. Corrupted disk blocks can be easily replaced by replicated blocks
from some other DataNode.
-------------------------------------

Replication
HDFS replication is critical for reliability, scalability, and performance. By default, HDFS has a replication factor of three. Therefore, Hadoop creators
have given careful thought on where each data block replica should be placed. It is all policy-driven. The current implementation follows the rack-aware 
replica management policy. Before we look at that in detail, we should first go through some of the facts about server racks. Any communication between 
two racks goes through switches, and the available network bandwidth between two racks is generally less than the bandwidth between machines on the same rack. 
Large Hadoop clusters spread across multiple racks. Hadoop tries to place replicas onto different racks. This prevents data loss in the case of an entire 
rack unit failing and utilizes multiple available rack bandwidth for reading data.
However, this increases write latency as data needs to be transferred to multiple racks. If the replication factor is three, then HDFS would put one replica 
on the local machine where the writer is present, otherwise it would put on a random DataNode. Another replica would be placed on the DataNode on a different
remote rack, and the last replica would be placed on another DataNode in the same remote rack. The HDFS replication policy makes an assumption that rack 
failures are less probable than node failures. A block is only placed in two different racks, not three, which reduces the probability of network bandwidth 
being used. The current replication policy does not equally distribute files across racks—if the replication factor is three, then two replicas will 
be on the same rack and the third one will be on another rack. In bigger clusters, we may have more replication factors. In such cases, two thirds of 
the replicas will be placed on one rack, one thirds of the replicas will be placed on another block, and the rest of the replicas will be equally 
distributed between the remaining racks.

The maximum number of replicas that we can have on HDFS is equal to the number of DataNodes because a DataNode cannot keep multiple copies of the same block. 
HDFS always tries for direct read requests to the replica that's closest to the client. If the reader node is under the same rack where the replica is, 
then it is assigned for reading the block. If the replication factor is more than the default replication factor, which is three, then the fourth 
and following replicas are placed randomly by sticking to the per rack replica limit. This can be calculated using the following formula:

(number_of_replicas-1)/number_of_racks+2
-----------------------------------------------

HDFS communication architecture
One of the important aspects of understanding HDFS is taking a look at how different components interact with each other programmatically and 
which type of network protocols or method invocations they use. All communication between HDFS components happens over the TCP/IP protocol.
Different protocol wrappers are defined for different types of communication. The following diagram represents such protocol wrappers, which we 
will look into in this section:



The preceding diagram can be explained as follows:

Client Protocol: This is a communication protocol that's defined for communication between the HDFS Client and the Namenode server. It is
a remote procedure call (RPC) that communicates with NameNode on a defined port using a TCP protocol. All user code and client-side libraries 
that need to interact with Namenode use this protocol. Some of the important methods in this protocol are as follows:
create: Creates a new empty file in the HDFS namespace
append: Appends to the end of file
setReplication: Sets replication of the file
addBlock: Writes additional data blocks to a file and assigns DataNodes for replication
There are multiple methods that are supported by the client protocol. All of these are with respect to client interaction with NameNode. 
We suggest you go over the Hadoop source code on GitHub to understand client and NameNode communication in more detail. This is the location 
of ClientProtocol.java in the Hadoop source code: hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java.
Data Transfer Protocol: The HDFS Client, after receiving metadata information from Namenode, establishes communication with Datanode to read 
and write data. This communication between the client and the Datanode is defined by the Data Transfer Protocol. Since this type of communication
does most of the data heavy lifting in terms of high volume read and writes, it is defined as a streaming protocol and is unlike the RPC protocol 
we defined earlier. Moreover, for efficiency purposes, the client buffers data up to a certain size of HDFS data blocks ( 64 MB by default) and 
then writes one complete block to the respective DataNodes. This protocol is mostly defined around data blocks. Some of the important methods 
that are included in this protocol are as follows:
readBlock: Reads a data block from a DataNode.
writeBlock: Writes a data block to a DataNode.
transferBlock: Transfers a data block from one DataNode to another.
blockChecksum: Gets the checksum value of a data block. This can be an MD5 or a CRC32 value.
The data transfer protocol is an important protocol that defines communication (read/write) between client and data nodes. Additional details 
about the methods it supports can be found in the Hadoop source code on GitHub. The following is the link for 
the same is: https://github.com/lpcclown/hadoop_enhancement/tree/master/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer.

Data Node Protocol: This is another important protocol that you should understand in detail. This protocol defines communication between 
Namenode and DataNode. The Data Node Protocol (DNP) is mostly used by Datanode to provide its operation, health, and storage information to Namenode. 
One of the important aspects of this protocol is that it is a one-way protocol. This means that all requests are always initiated by DataNode.
NameNodes only respond to requests that are initiated by DataNodes. Unlike the previous protocol, this is a RPC protocol that's defined over TCP. 
The following are some of the important methods that are included in this protocol:
registerDatanode: This registers new or rebooted data nodes to NameNode.
sendHeartbeat: This tells NameNode that the DataNode is alive and working properly. This method is not only important from the point of view of
knowing which DataNode is alive, but it also gives NameNode a chance to respond back to DataNodes with a set of commands that it wants them to execute.
For example, at times, NameNode wants to invalidate some of the blocks stored in data nodes. In that case, in response to the sendHeartbeat() method,
NameNode sends an invalidate block request to the DataNode. 
blockReport: This method is used by DataNode to send all of its locally stored block-related information to NameNode. In response to this, 
NameNode sends DataNodes blocks that are obsolete and should be deleted.
----------------------------------------

